<job_conf>

     <plugins workers="4">
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner"/>
     </plugins>

    <destinations default="local">
        <destination id ="local" runner="local"/>
        <destination id="slurm" runner="slurm" />
        <!-- All tools shall collect the job destination parameters from the tool menu.
                  This is done by the following dynamic runner : destinations.py
         -->
        <destination id="generate_slurm_job_params" runner="dynamic">
            <param id="type">python</param>
            <param id="function">integrate_job_destination_params</param>
        </destination>
        
        <!-- Open to users with "run_anayse_and_plot"  role-->
        <destination id="analyseandplot" runner="dynamic">
            <param id="type">python</param>
            <param id="function">analyse_and_plot</param>
        </destination>
        
         <!-- Open to users with "run_statistics" role  -->
        <destination id="statistics" runner="dynamic">
            <param id="type">python</param>
            <param id="function">statistics</param>
        </destination>
        
        <!-- This is a destination with fixed destination params - used to test connection to cluster -->
        <destination id="slurm_static" runner="slurm">
          <param id="nativeSpecification">--time=05:00 --mem-per-cpu=2</param>
        </destination>
	
        <destination id="interactive_slurm" runner="slurm">
            <param id="nativeSpecification">--time=48:00:00 --mem-per-cpu=8</param>
            <param id="docker_enabled">true</param>
            <param id="docker_volumes">$defaults,/storage/shared/data:ro,/storage/shared/software:ro</param>
            <param id="docker_sudo">false</param>
            <param id="docker_net">bridge</param>
            <param id="docker_auto_rm">true</param>
            <param id="docker_set_user"></param>
            <param id="require_container">true</param>
        </destination>


	<destination id="interactive_slurm" runner="dynamic">
	    <param id="function">jupyter</param>
        </destination>


	<destination id="interactive_slurm_general" runner="slurm">
            <param id="nativeSpecification">--time=48:00:00 --mem-per-cpu=8</param>
            <param id="docker_enabled">true</param>
            <param id="docker_volumes">$defaults</param>
            <param id="docker_sudo">false</param>
            <param id="docker_net">bridge</param>
            <param id="docker_auto_rm">true</param>
            <param id="docker_set_user"></param>
            <param id="require_container">true</param>
        </destination>

     </destinations>
     <tools>    
       <tool id="interactive_tool_jupyter_notebook_fys5555" destination="interactive_slurm"/>
       <tool id="interactive_tool_comphep" destination="interactive_slurm_general"/>
       
       <tool id="analyseAndPlot" destination="analyseandplot" />
       <tool id="plotHistograms" destination="slurm_static" />
       <tool id="polarQuestAnalysis" destination="slurm_static" />
       <tool id="statistics" destination="statistics"/>
       <tool id="SUSYPheno" destination="slurm_static"/>
     </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">5</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">5</limit>
        <limit type="destination_user_concurrent_jobs" tag="slurm">10</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">5</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">10</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">168:00:00</limit>
        <!-- total_walltime:
                Total walltime that jobs may not exceed during a set period.
                If total walltime of finished jobs exceeds this value, any
                new jobs are paused.  `window` is a number in days,
                representing the period.
         -->
        <limit type="total_walltime" window="7">168:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">50GB</limit>
    </limits>
    
    <resources>
        <group id="dynamic_slurm_job_params">memory,time</group>
   </resources>
    
 </job_conf>
